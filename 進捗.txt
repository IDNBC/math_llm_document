5月25日
https://colab.research.google.com/drive/1CKEn1XvMYoijL5dEWzu2d1yvlWhlAuM_?usp=sharing
このGoogle Colabで数式データを少量だけ使い、Transformers ライブラリによる LLM の微調整を行った。
dataset.select(range(1000))とdataset.select(range(5000))の2つで実験してみました。
1000でやったときは、5+3の回答が3.5という結果になった。50+10も50になるなど間違いが観られた。
5000では学習時間が体感で数倍～10倍くらいになったと思います。
input_text = "1 + 1 ="に対して、
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
1 + 1 = 2.5$
このように出力しました。
Flaskを起動して保存したモデルに応答させるようにしたところ、答えは間違っていましたが入力に対してきちんと回答を送ってくるという動作は確認できました。
今回はTransformerライブラリを使った微調整だったので、次はPyTorch学習まで実際に行い、精度のいい回答を出力できるようにデータなどを工夫していきます。
モデルも一緒にGitHubのリモートリポジトリへプッシュしようとしたら出来ないといわれ、単体で100MBを超えるファイルは無理らしいとAIから聞きました。
https://huggingface.co/nobuchiyo345/first-model
このHugging Faceの自分のページに訓練したモデルをアップロードするところまでやりました。

5月26日
本日は簡単なデータセットを作りました。四則演算だけといっても2項だけの計算でも1桁同士や、1桁と2桁、2桁と1桁などがあります。
また負の数を混ぜた場合に掛け算と割り算でも考えたり、小数を含めた場合といろいろあります。
差しあたっては本日作ったもので学習してみるとして、この辺のデータ設計もしっかり練らねばなりません。
以前にPyTorchの練習用で作った仮想環境とは別に作品制作のための仮想環境を構築するのに、
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
このコマンドを実行しました。PyTorchの公式サイトに載っていたものです。
インストール済みのPyTorchのバージョンは、
pip show torch
で確認できます。
自宅のPCではグローバルにインストールしているので、仮想環境は構築しません。

5月27日
https://dreamer-uma.com/beginner-how-to-use-pytorch/
ここのサイトもなかなか勉強になりそう。
単回帰モデルの場合ですが、PyTorchのフレームワークを使わないで実装しています。
自宅の環境で上記のサイトのコードを動かしてみると、警告が出ました。

\torch\utils\_pytree.py:185: FutureWarning: optree is installed but 
the version is too old to support PyTorch Dynamo in C++ pytree. C++ py
これは一部ですが、optreeがインストールしてあるけれどもバージョンがとても古いという。
AIによると、import torchが実行されるとPyTorchの内部で torch.utils._pytree がインポートされる。
C++最適化のために optree を探しに行く。optree のバージョンが古い → 警告を出す。
という流れで起きたようです。
ちなみに警告がどの行を起因として発生したかを調べるには、
import warnings
warnings.simplefilter("default")
import torch
print("torch version:", torch.__version__)
このコードを先頭に書いておくといいらしい。torchは今回の場合に使っただけで、一般に調べる場合にはwarningsの方だけでよいと思います。

借りてきたPyTorchの解説書に正しいインストールの仕方が載っていました。
公式webページからget startをクリックし、使っているOSなどを選択し、表示されるコマンドを使ってインストールするのが正しい手順のよう。
自宅のPCではStable(2.7.0)、Windows、Pip、Python、CPUを選びました。
実習用PCでもGPUを使う段階になったら仮想環境で正しい手順でインストールしなおして。


5月28日
PyTorchについて理解を深めるために『動かしながら学ぶPyTorchプログラミング入門』を読み始める。
PyTorchのtensorやNumPyのndarrayの相互変換する意義など詳しく解説されていた。

PLaMO-2.0-primeに目的のLLMを作るまでの工程を聞いたら、
モデルの規模: 6層のTransformer、埋め込みサイズ512、アテンションヘッド数8。
データ量: 1000万トークン程度の四則演算問題と解答のペア。
学習時間: GPUリソースの制約に応じて、数週間から数ヶ月。
という回答があった。
学習時間を大幅に減らしたい場合はどうすればいいか聞くと、
層の数を減らす。埋め込みサイズや隠れ層のサイズを小さくする。アテンションヘッド数を減らす。
データセットの量を減らす。学習率を高め(1e-2から1e-3)に設定する。高速な収束が期待できるオプティマイザを使用する。
GPUメモリが許す限りバッチサイズを大きくする。エポック数(5～10程)を削減する。
損失が改善しなくなった時点で学習を停止する。混合精度トレーニング(FP16)を使用する。
高速なトークン化アルゴリズムを使用する(sentencepieceやbyte-pair encoding)。

学習に時間がかかるとしてもチェックポイントを保存することで後から学習を再開することもできるようです。
https://chat-demo.plamo.preferredai.jp/c/61b547dd-b1e9-4292-9eef-bffe6cc5e4ac
「chat-PyTorchで軽量LLMを作る手順.txtの」ファイルにこのチャットでのやり取りを出力してあります。

5月29日
PLaMO-2.0-primeでは他のAIチャットサービスには見られない特徴がありました。
一つはAIとの1回のやり取りをコピーするのでなく、チャット全体をテキストファイルなどの形式で保存できること。
もう一つはユーザがテキストを入力する欄にLaTex形式で入力すると、内容を確定後(LLMへの送信後)に数式がレンダリングされて表示される点。
これはとてもいい機能なので時間があったら実装してみたい。

6月1日
LLMのモデルアーキテクチャについて学びました。
モデルアーキテクチャはニューラルネットワーク内部で、つまりデータがモデル内をどのように伝播するかを定めたもの。
その本質的な役割は、情報処理の流れ(順伝播/逆伝播)、計算効率(並列処理)、知識保持(文脈保持)、汎化性能(未知データへ対応)にある。
言語モデルではTransfomerが主流になっている。
LLMは大量のテキストを入力して意味の通った出力をする必要がある。そのためには以下が必要。
1.文脈を長く保持できる(Self-Attentionが有効)
2.並列計算が可能(Transformerの構造が向いている)
3.スケーラビリティが高い(数千億パラメータでも学習可能)

アーキテクチャ選定は、どこまで高性能なモデルを効率的に構築できるかに直結する。

[進化の系統樹]
RNN/LSTM時代（2014）
　↓
Transformer革命（2017）
　├─ Encoder系（BERTなど）
　├─ Decoder系（GPTなど）
　└─ ハイブリッド（T5など）
　　　↓
効率化時代（2020~）
　├─ スパース化（Switch Transformer）
　├─ 状態保持型（RetNet）
　└─ 量子化対応（BitNet）

AIにモデルアーキテクチャの選定から学習までのステップを聞いたところ、次のよう回答があった。
詳しくは、chat履歴か出力したテキストファイル「chat-数学LLM開発のステップ.txt」を参照すること。

ステップ1
LLMにおこなわせたいタスクを明確し要件定義する。
今回は四則演算をし、LaTex形式で出力すること。
軽量なモデルにするので計算リソースは限られている(長くても分割して数日で学習が終える程度)。

ステップ2
タスクに最適なアーキテクチャを選ぶ。
Transformerは長距離依存関係の学習に優れ、並列化が可能。
演算記号や数値間の関係性を捉えるのに、Transformerの自己注意機構が適している。

LSTM(Long Short-Term Memory)は、時系列データの処理に適し比較的計算コストが低い。
長距離依存関係の学習が苦手なので、計算リソースが限られている場合などに使う。

ステップ3
選択したアーキテクチャの具体的な設計を行う。
設計は以下4つのことを含む。
埋め込み層で入力トークンを数値ベクトルに変換する。
自己注意層で系列内の各要素間の関係性を学習する(Transformer)。
フィードフォワード層に非線形変換を導入する。
出力層では数式のLaTex表現を予測するための層を設計する。

ステップ4
モデルの学習に必要なデータセットを準備する。
入力をトークン化するためのトークナイザーを使用する。

ステップ5
モデルの学習に必要な設定を行う。損失関数、オプティマイザ、学習率など。

ステップ6
モデル評価とチューニングを行う。
評価指標としては数式計算の精度を確認するなど。
ハイパーパラメータチューニングでは学習率、バッチサイズ、モデルサイズなどを調整する。

