AIの示した手順

ステップ1.PyTorchの基礎を固める
1-1.PyTorchのインストール
これは練習用に参考にしたサイトでのバージョンを仮想環境内でインストールした。
実際に構築する際には改めて仮想環境を構築し、その環境内でインストールする。

1-2.テンソルとその操作の理解
x = torch.empty(5, 3)
print(x)
で初期化されていないメモリ上の値を入れて、5×3行列を生成できる。

x = torch.rand(5, 3)
print(x)
torch.rand(5, 3) は、[0, 1) の範囲の一様乱数で初期化された 5×3 の行列を生成する。

x = torch.zeros(5, 3, dtype=torch.long)
print(x)
これはlong型の指定があり、.zeros()メソッドで要素の値が0の行列を生成する。
torch.longはPyTorchにおける64ビット整数型(int64)。

x = torch.tensor([5.5, 3])
print(x)
これは数値を直接指定した場合で1次元のテンソル、すなわちベクトルとして[5.5, 3]の入ったものを生成する。
3はint型であるが、5.5がfloatなので、PyTorch は全体をfloat型(デフォルトでfloat32)に統一される。

テンソルはTorch TensorからNumPy Arrayへ、またその逆への変換が可能。

1-3.自動微分(Autograd)の仕組み
PyTorchによるニューラルネットワーク構築の土台となっているのが、自動微分パッケージです。
torch.Tensorはパッケージの中心的なクラスです。
.requires_grad属性がTrueに設定された場合、autogradパッケージによってすべての操作が追跡され、
演算が終了した際は.backward()を呼び出すことで、すべての操作に対する勾配が自動的に計算されます。
https://colab.research.google.com/drive/1GSD5mP42qWeIK05mY_OhNkp5h6tp6xtS#scrollTo=DFJfUAncKLLQ
ここでコードを実行して動作を確認しました。

1-4.ニューラルネットワークの構築
torch.nnパッケージを利用することで、ニューラルネットワークを構築できる。
nnはモデルを定義したり、モデルに微分を適用するために、autogradに依存している。


1-5.損失関数

1-6.最適化アルゴリズム

ステップ2.LLMの基礎知識を習得する

2-1.自然言語処理の基礎

2-2.LLMのアーキテクチャ

2-3.Embedding層の役割

2-4.Attention機構の理解

ステップ3.簡単なLLMモデルをPyTorchで実装する

3-1.トークナイザーの実装または選定

3-2.Embedding層の定義

3-3.Transformerブロックの構築

3-4.LLMモデル全体の構築

ステップ4.学習データの準備

4-1.四則演算のデータ生成

4-2.データのトークン化

4-3.データローダーの作成

ステップ5.モデルの学習

5-1.モデル、損失関数、最適化アルゴリズムの定義

5-2.学習ループの実装

5-3.学習の進捗のモニタリング

5-4.モデルの保存

ステップ6.モデルの評価

6-1.評価データの準備

6-2.評価ループの実装


ステップ7.さらなる発展
モデルアーキテクチャの改良: Transformerの層数を増やしたり、Attention機構をMulti-Headにしたりするなど、モデルの複雑さを増して性能向上を目指す。																			
ハイパーパラメータの調整: 学習率、バッチサイズ、Embedding次元数など、様々なハイパーパラメータを調整し、最適な値を見つける。																			
より複雑な演算規則の学習: ( ) を含む式の優先順位、文字式の扱い、同類項の整理など、より複雑な数学的な規則を学習させる。																			
数式表示の改善: モデルが出力する数式がLaTeX形式になるように学習データを工夫したり、出力後の後処理を検討する。																			
UIの実装: FlaskなどのWebフレームワークとHTML/CSS、JavaScriptを用いて、ブラウザ上でLLMと対話できるUIを構築する。
MathJaxやKaTexを組み込み、数式を綺麗に表示させる。


PyTorchのソースコード解説
PyTorchのニューラルネットワークは状態(重み)を持つ必要があり、nn.Moduleを継承する必要がある。
