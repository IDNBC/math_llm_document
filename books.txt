LLM構築のためにPyTorchの使い方などを参考にした。

書籍名：[ディープラーニング実装入門 PyTorchによる画像・自然言語処理 (impress top gearシリーズ) ]

1.ニューラルネットワーク
教師あり学習のアルゴリズム。
微分可能な構造をつなげていき柔軟なネットワーク構造の設計を可能にする。
ノードをつなぎ合わせた層を持つ構造になっている。
各ノード間を全てつなぐので全結合層という。
入力層は入力変数の数だけノードを用意する。
この入力層にはバイアス項の分も用意するのが一般的。
出力層は分類したカテゴリの数だけノードを用意する。
中間層では入力された変数を処理して、より分類するために有利になるような値を得るようにする。
2層で1セットと考えることで複雑になっても議論が簡単になる。

2.順伝播
入力された値から予測値を計算し順方向に伝播させること。

3.線形変換と非線形変換
wは重み、bはバイアス、uは線形変換、hは非線形変換。
線形変換では重みと入力変数の線形結合を行う。
これだけだと線形な関係性を持つ場合だけにしか関係性を表現できない。
入出力に非線形な関係があるときは非線形変換を施して、層の積み重なったニューラルネットワーク全体に非線形性を持たせる。
非線形変換を行う関数を活性化関数という。
線形変換uを施した出力が非線形変換を施す層に入力として入ると、非線形変換をaとしたら
h = a(u)
のように表される。

4.代表的な活性化関数の例
[ロジスティックシグモイド関数]
入力値uに基づき0～1の間の数を返す。この関数を採用すると勾配消失という現象で学習が進行しなくなる問題が起きやすい。
σ'(x) = d/dx (1/(1+e^{-x})) = σ(x)(1-σ(x))
という形でシグモイド関数の勾配(導関数)が求まる。元の関数を使って導関数を記述できる。
この右辺の積は、a(0 < a < 1)という数に対して、a(1-a)の最大値を考える。
これはaについての2次関数でf(a) = a(1-a)
これを展開して平方完成させるやり方と微分で求めるやり方がある。
f(a) = a - a^2
微分して、
f'(a) = 1 - 2a
これを0と置いて解き、最小か最大かを確認する。
0 = 1 - 2a
2a = 1
a = 1/2
で、元の式の2階微分で極大か極小かが判別でき、
f''(a) = -2 < 0
より、負なので極大とわかる。そうするとa = 1/2を代入して、
f(1/2) = 1/2 - 1/4 = 2/4 - 1/4 = 1/4 = 0.25
が出てくる。これで勾配は最大でも0.25ということになる。
多層のニューラルネットワークでは、誤差(損失関数の勾配)を各層に逆向きに伝播させる。
各層の勾配はチェインルール(連鎖葎)で計算され、チェインルールの式中にシグモイド関数(活性化関数)の導関数が使われるとする。
各層で勾配が最大でも0.25でも、ほとんどはもっと小さい値なので全体の勾配計算は最大でも(0.25)の掛け合わせになる。
層が深くなる、つまり層の数が多くなると勾配はほぼ0に近づいていく。
これが勾配消失(vanishing gradient)である。
その結果浅い層まで誤差が伝わらず、初期層の重みが更新されないので意味のある特徴を学習できない。

[ReLU関数]
シグモイド関数の欠点を回避するために正規化線形関数(rectified linear unit:ReLU関数)が使われる。
max(0, u)は0とuを比較して大きいほうを返す関数として定義されるのがReLU関数。
入力が負の場合は0で一定、正の場合は入力をそのまま出力する。
シグモイド関数では、入力が0から遠いほど勾配が小さくなるがReLU関数では入力値が正なら勾配は一定となる。
この性質が勾配消失を回避させる。




この書籍の8章8節では文章生成を扱っている。
簡単な足し算のデータセットを用意して、Seq2Seqというモデルで学習し、精度を確認するところまで行っている。
