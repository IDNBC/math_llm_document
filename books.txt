LLM構築のためにPyTorchの使い方などを参考にした。

書籍名：[ディープラーニング実装入門 PyTorchによる画像・自然言語処理 (impress top gearシリーズ) ]

1.ニューラルネットワーク
教師あり学習のアルゴリズム。
微分可能な構造をつなげていき柔軟なネットワーク構造の設計を可能にする。
ノードをつなぎ合わせた層を持つ構造になっている。
各ノード間を全てつなぐので全結合層という。
入力層は入力変数の数だけノードを用意する。
この入力層にはバイアス項の分も用意するのが一般的。
出力層は分類したカテゴリの数だけノードを用意する。
中間層では入力された変数を処理して、より分類するために有利になるような値を得るようにする。
2層で1セットと考えることで複雑になっても議論が簡単になる。

2.順伝播
入力された値から予測値を計算し順方向に伝播させること。

3.線形変換と非線形変換
wは重み、bはバイアス、uは線形変換、hは非線形変換。
線形変換では重みと入力変数の線形結合を行う。
これだけだと線形な関係性を持つ場合だけにしか関係性を表現できない。
入出力に非線形な関係があるときは非線形変換を施して、層の積み重なったニューラルネットワーク全体に非線形性を持たせる。
非線形変換を行う関数を活性化関数という。
線形変換uを施した出力が非線形変換を施す層に入力として入ると、非線形変換をaとしたら
h = a(u)
のように表される。

4.代表的な活性化関数の例
[ロジスティックシグモイド関数]
入力値uに基づき0～1の間の数を返す。この関数を採用すると勾配消失という現象で学習が進行しなくなる問題が起きやすい。
σ'(x) = d/dx (1/(1+e^{-x})) = σ(x)(1-σ(x))
という形でシグモイド関数の勾配(導関数)が求まる。元の関数を使って導関数を記述できる。
この右辺の積は、a(0 < a < 1)という数に対して、a(1-a)の最大値を考える。
これはaについての2次関数でf(a) = a(1-a)
これを展開して平方完成させるやり方と微分で求めるやり方がある。
f(a) = a - a^2
微分して、
f'(a) = 1 - 2a
これを0と置いて解き、最小か最大かを確認する。
0 = 1 - 2a
2a = 1
a = 1/2
で、元の式の2階微分で極大か極小かが判別でき、
f''(a) = -2 < 0
より、負なので極大とわかる。そうするとa = 1/2を代入して、
f(1/2) = 1/2 - 1/4 = 2/4 - 1/4 = 1/4 = 0.25
が出てくる。これで勾配は最大でも0.25ということになる。
多層のニューラルネットワークでは、誤差(損失関数の勾配)を各層に逆向きに伝播させる。
各層の勾配はチェインルール(連鎖葎)で計算され、チェインルールの式中にシグモイド関数(活性化関数)の導関数が使われるとする。
各層で勾配が最大でも0.25でも、ほとんどはもっと小さい値なので全体の勾配計算は最大でも(0.25)の掛け合わせになる。
層が深くなる、つまり層の数が多くなると勾配はほぼ0に近づいていく。
これが勾配消失(vanishing gradient)である。
その結果浅い層まで誤差が伝わらず、初期層の重みが更新されないので意味のある特徴を学習できない。

[ReLU関数]
シグモイド関数の欠点を回避するために正規化線形関数(rectified linear unit:ReLU関数)が使われる。
max(0, u)は0とuを比較して大きいほうを返す関数として定義されるのがReLU関数。
入力が負の場合は0で一定、正の場合は入力をそのまま出力する。
シグモイド関数では、入力が0から遠いほど勾配が小さくなるがReLU関数では入力値が正なら勾配は一定となる。
この性質が勾配消失を回避させる。




この書籍の8章8節では文章生成を扱っている。
簡単な足し算のデータセットを用意して、Seq2Seqというモデルで学習し、精度を確認するところまで行っている。


書籍名：[動かしながら学ぶPyTorchプログラミング入門]
1.[テンソル]
テンソル(Tensor)は任意の多次元配列で階(ランク)という概念がある。
0階のテンソルはスカラーで整数など一般的な値のこと。
1階のテンソルはベクトルで1次元配列、2階のテンソルが行列で2次元配列。
3階になると2次元配列を並べたものになり、それ以上の階数も作れる。
PyTorchではテンソルを扱うためにtorch.Tensor関数が用意されている。
この関数で生成したテンソルには値の他にも形状やデータ型などの情報が格納される。
x = torch.tensor([1,2,3],[4,5,6], dtype=torch.float64)は64ビット浮動小数点数指定で生成
print(x.size())で形状の確認
x = torch.arrange(0, 10)で0～9までの1次元配列生成
x = torch.linspace(0, 10, 5)は0～10までを5つに分けた値で生成
x = torch.rand(2, 3)で2×3テンソルに0～1の間の乱数を生成
x = torch.zeros(2, 3)で2×3テンソルの全ての要素が0となる
x = torch.ones(2, 3)で要素がすべて1のテンソルに

x = torch.tensor([1,2,3]).to('cuda')でtoメソッドを使用してテンソルをGPUに転送

x = torch.tensor([1,2,3],[4,5,6])でテンソルをつくり、
x_reshape = x.view(3,2)で2×3テンソルがview関数で3×2テンソルに形状が変更される。

テンソルの演算で気を付けたいこと。
数学で扱う一般的な行列と異なり、Tensor同士の演算は対応する要素同士の積や商を計算する。



2.[PyTorchのimport]
PyTorchのインストールができていても、それだけではPyTorchの関数を使うことができない。
Pythonでは組み込みの関数以外を使用するにはそれらをまとめたパッケージを読み込む(インポート)必要がある。

3.[Tensorとndarrayの変換]
ディープラーニング実装で前処理は重要なステップ。
前処理によく使われるpandasやscikit-learnはTensorをサポートしていない。
PyTorchのTensorよりもNumPyのndarrayをサポートしているライブラリのほうが多い。
前処理ではデータ構造としてndarrayを、ディープラーニング実行時にはTensorに変換したいとする。
PyTorchにはそれらを変換しあうための関数が用意されている。

tensor = torch.from_numpy(array)でndarrayからTensorへ変換
tensor2array = tensor.numpy()でTensorからndarrayへ変換

4.[自動微分 autograd]
ニューラルネットワークの最適なパラメータ(重みのこと)を算出するために入力と出力の誤差が最小になるようにしたい。
損失関数が最小になるようなパラメータを算出することになり、そこで微分が必要になる。
PyTorchにはautogradパッケージがあり、テンソルの計算を自動で微分する機能がある。
Tensorにはrequires_gradという属性があり、この属性がTrueの時、自動微分機能が有効になる。
自動微分が有効なとき、Tensorは勾配情報を保持する。
書籍ではテンソルとしてスカラーを3つ(a, b, x)用意し、y = ax + bを計算し、backwardメソッドで勾配を算出している。
a=3, b=4, x=5とすると、 y = ax + bについて
y.backward()でyに対して微分し勾配を算出
yをaで微分すると、aは1にbは定数で消え、xは残る。よって、1 * x + 0 = 5
yをbで微分すると、aとxは定数扱いで消え、bは1になる。よって、0 * 0 + 1 = 1
yをxで微分すると、xは1にbは定数で消え、aは残る。よって、a * 1 + 0 = 3

この書籍ではデフォルトではTrueになっているという説明文があったが、AIによるとtorch.tensor()ではデフォルトでrequires_grad=Falseという。
