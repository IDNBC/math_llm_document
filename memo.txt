PyTorchではtorchtextを利用して辞書を作成できる。
各単語に割り振られたIDはstoiで確認できる。field_x.vocab.stoi
単語が辞書内のIDに置き換わることをインデックス化という。
Embedding層には入力値の単語を分散表現して特徴抽出する目的がある。
変換前の系列データ(入力値)から文脈などの情報を圧縮したベクトルを生成し、それを特徴ベクトルという。
特徴ベクトルは次の層に渡される。
分散表現前のone-hot表現された入力値は長さ数万のベクトルの内、1個だけ1で他は0という疎な行列を形成する。
このままだと各単語の特徴が表現されず、そのままでネットワークに入力しても訓練が適切に行われない。
単語ごとの意味を特徴抽出する必要があり、考案されたのが分散表現。


Pythonでは辞書と同じくkeyとvalueを持ったJSON形式でデータのやり取りを行うのが一般的。
Flaskにはjsonifyという関数があり、これを使って辞書型の変数を格納する。

リクエストにはGETメソッドとPOSTメソッドがある。
POSTメソッドは情報をURLに付加して送る。
GETメソッドは情報をBodyという直接見えない内部に含めて送る。

Pythonにはrequests渡欧ライブラリがあり、これでサーバーにリクエストを送れる。
import requests
url = 'http://<ipアドレス>:5000/'

res = requests.get(url)
res

入力値xの情報を付けてリクエストを送りたい場合がある。
値付きの場合は一般にPOSTメソッドを使用する。
POSTメソッドだとSSL/TLSで暗号化できる利点がある。



[PyTorch入門]


[最適化]
ハイパーパラメータ
ハイパーパラメータは、モデルの最適化プロセスを制御するためのパラメータです。
ハイパーパラメータの値が異なると、モデルの学習や収束率に影響します。

epochはイテレーションの回数
batch sizeはミニバッチサイズを構成するデータ数
learning rateはパラメータ更新の係数で、値が小さいと変化が少なく、大きすぎると学習に失敗しやすい

learning_rate = 1e-3
batch_size = 64
epochs = 5
このように設定する。

ハイパーパラメータを設定後、訓練で最適化のループを回すことで、モデルを最適化します。
最適化ループの1回のイテレーションは、エポック(epoch)と呼ばれます。
各エポックでは2種類のループから構成されます。
・訓練ループ：データセットに対して訓練を実行し、パラメータを収束させます
・検証 / テストループ：テストデータセットでモデルを評価し、性能が向上しているか確認します

損失関数：Loss Function
データが与えられても、訓練されていないネットワークは正しい答えを出力しない可能性があります。
損失関数はモデルが推論した結果と、実際の正解との誤差の大きさを測定する関数です。訓練ではこの損失関数の値を小さくしていきます。
損失を計算するためには、入力データに対するモデルの推論結果を求め、その値と正解のラベルとの違いを比較します。

ここまでで正解という言葉が出てきたが、これは教師あり学習を前提とした説明のようで、正解データも用意しなければならない。

一般的な損失関数としては、回帰タスクではnn.MSELoss(Mean Square Error)、分類タスクではnn.NLLLoss(Negative Log Likelihood) が使用されます。
nn.CrossEntropyLossは、nn.LogSoftmax と nn.NLLLossを結合した損失関数となります。
モデルが出力するlogit値をnn.CrossEntropyLossに与えて正規化し、予測誤差を求めます。
最適化は各訓練ステップにおいてモデルの誤差を小さくなるように、モデルパラメータを調整するプロセスです。

訓練したいモデルパラメータをoptimizerに登録し、合わせて学習率をハイパーパラメータとして渡すことで初期化を行います。
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

訓練ループ内で、最適化（optimization）は3つのステップから構成されます。

[1] optimizer.zero_grad()を実行し、モデルパラメータの勾配をリセットします。
勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットします。

[2] 続いて、loss.backwards()を実行し、バックプロパゲーションを実行します。
PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求めます。

[3] 最後に、optimizer.step()を実行し、各パラメータの勾配を使用してパラメータの値を調整します。

PyTorchでは、loss.backward() を実行すると、既存の勾配に対して「加算」されるようになっています。
これは**累積型（accumulative）**の動作です。
つまり、複数回 backward() を呼ぶと、勾配がどんどん蓄積されていきます。
PyTorchは柔軟性を重視しているため、以下のようなケースに対応できるように自動でリセットしません。
例：大きなデータセットを何回かに分けて処理し、その合計勾配を1回で適用したいとき（いわゆるgradient accumulation）。

optimizer.zero_grad()     # [1] 前回の勾配をリセット
loss.backward()           # [2] 今回の損失に対する勾配を計算
optimizer.step()          # [3] 勾配に従ってパラメータを更新

この順序で実行しないと前回の勾配が残ったままになり、意図しない更新を行う。

[モデルの保存と読み込み]
PyTorchのモデルは学習したパラメータを内部に状態辞書（state_dict）として保持しています。
これらのパラメータの値は torch.save を使用することで、永続化させることができます。

model = models.vgg16(pretrained=True)
torch.save(model.state_dict(), 'model_weights.pth')
このmodel_weights.pthという名前のファイルになるが、これはPyTorchで作るときの拡張子がついている。

モデルの重みを読み込むためには、予め同じモデルの形をしたインスタンスを用意します。
そしてそのインスタンスに対してload_state_dict()メソッドを使用し、パラメータの値を読み込みます。

model = models.vgg16() # pretrained=Trueを引数に入れていないので、デフォルトのランダムな値になっています
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()

ドロップアウトやバッチノーマライゼーションレイヤーをevaluationモードに切り替えるために、推論前には model.eval()を実行することを忘れないようにしてください。
これを忘れると、推論結果が正確ではなくなります。

モデルの重みをロードする場合は、先にモデルのインスタンスを用意する必要があります。
モデルクラスの構造も一緒に保存したい場合もあるかと思います。
その際は保存時に、model.state_dict()ではなくmodelを渡します。

# 保存時
torch.save(model, 'model.pth')
# 読み込み時
model = torch.load('model.pth')

