1.[仮想環境]
なぜ仮想環境を構築するかというと、
Python の venv（Virtual Environment）は、
システム全体（グローバル環境）から独立した、プロジェクト専用の Python 実行環境を作る仕組みです。
つまり「このプロジェクトだけで使うパッケージを分離して管理できる」ということが目的です。
仮想環境を作成すると、指定した(カレントディレクトリ)フォルダ内に venv/ というディレクトリが作られます。
venvフォルダに入っているPyTorchなどのパッケージはこの仮想環境の中でしか使えない。プロジェクトごとに必要なパッケージのバージョンが違うことがあり、そうしたときに仮想環境があると分離できるので便利である。
仮想環境（venv）は本質的にはPython や pip の実行ファイルの参照先（パス）を切り替えているだけ。
仮想環境を有効化すると、環境変数 PATH が一時的に書き換えられ、先に venv/bin/（またはWindowsなら venv\Scripts\）が見られるようになります。
だから、実際には「最初に見つかった python や pip を使う」だけの話です。

まとめると、
仮想環境で依存関係を隔離する	どのフォルダの Python を使うか切り替えてるだけ
仮想環境を有効化する	python コマンドの参照先を一時的に変える
パッケージのインストール先が分かれる	pip が使うフォルダが変わってるだけ


2.[トークナイザー]
トークナイザー（tokenizer）は、AI、特に自然言語処理モデルにおいて極めて重要な役割を担います。
学習対象の文章（テキストデータ）を単語や部分語（サブワード）単位で分割し、それぞれのトークンにID（整数値）を割り振るのが主な機能です。
このID割り当てという性質は、文字コード（例：UTF-8）と似ています。
ただし、文字コードが「1文字＝固定ID」の対応を行うのに対し、トークナイザーでは意味的・頻度的な観点から、可変長のトークン単位で分割し、使用頻度に基づいたIDを動的に割り当てる点が大きく異なります。

トークナイザーによって出力されたID列は、モデルの Embedding層 を通じて高次元のベクトル空間に変換されます。
この空間では、意味的に近いトークン同士（例："dog" と "puppy"）は近い位置に、意味の異なるトークン（例："dog" と "banana"）は離れた位置に配置されるよう、学習中に最適化されます。
例：["I", " love", " math", "."] → [40, 1802, 1186, 13]
Embeddingベクトルはモデルの最初の層であり、**「このトークンIDはこの意味を持つ」**という知識の出発点です。
このため、トークナイザーの語彙（トークンとIDの対応）と、モデルのEmbedding行列（IDとベクトルの対応）は完全に一致していなければなりません。

Embeddingベクトルはモデルの最初の層であり、**「このトークンIDはこの意味を持つ」**という知識の出発点です。
このため、トークナイザーの語彙（トークンとIDの対応）と、モデルのEmbedding行列（IDとベクトルの対応）は完全に一致していなければなりません。
そのため、推論時にも同じトークナイザーを使って同じ規則でID列を生成しなければ、モデルが期待する「意味」がズレてしまうのです。
仮に異なるトークナイザーを使った場合、同じ文字列でも異なるID列に変換されてしまい、モデルが学習した意味と噛み合わず、出力が崩壊することになります。
これはちょうど、文字コードの異なるファイルを誤った文字コードで開いたときに発生する「文字化け」とよく似ています。
したがって、学習時と推論時で必ず同じトークナイザーを使用することが原則です。

Hugging Face の Transformers ライブラリでは、モデルとそのトークナイザーが常にセットで扱われます。
たとえば .from_pretrained("model-name") を使用すると、対応するトークナイザーと語彙表も一緒に取得される仕組みになっています。
これにより、トークナイズの不一致による意味の崩壊を防いでいます。

トークンへの分割とIDの割り当ては、主に以下のようなステップで行われます（Byte-Pair Encoding などの手法の場合）：
(1).頻出する文字や部分語のペアを検出
(2).頻出ペアをマージして語彙表を構築（頻度の高い連結語を優先）
(3).完成した語彙表に対してIDを割り振る（通常は頻度順）
このプロセスにより、頻繁に使われるトークンはより短いIDで表現され、学習効率が上がるよう設計されています。
※他の手法（WordPiece, Unigramなど）も似た考え方で語彙最適化を行います。

3.[トークナイザー、Embedding層と学習の関係]
トークナイザーがトークン ↔ ID の対応を、Embedding層が ID ↔ 意味ベクトル の対応を担っており、モデルの学習はこの2段階の対応を通じて意味理解を構築する。よって、両者の整合性が絶対に必要である。

割り振られたID列について先頭から順に行列の各行ベクトルに対応するようにしてembedding行列が作られる。
Embedding行列を構成するのは疎なベクトル（＝ほとんど0）ではなく、初期化時に通常ランダムな実数値で埋まっていて、学習を通して意味的に整ったベクトルになります。
入力ベクトルは ワンホットベクトルを使ってEmbedding行列と乗算する という形式でも理解できますが、実装上は単に該当行を取り出す形で高速化されます。

学習により、損失関数が計算され誤差逆伝播によってモデルの最初の層であるembedding層も更新される。
この時、使われたトークンの行だけが更新され他は変わらない。

トークナイザーを構築する際にも、語彙的に多様で十分な規模のデータセットが必要です。ただし、これは一度だけ行われる処理で、目的は効率的な分割ルールと語彙表の構築です。
トークナイザーは事前に決めたルールやアルゴリズム（BPE, WordPiece, SentencePieceなど）で一度だけ構築されます。
一方でモデル本体の学習では、より大規模で意味的文脈を豊富に含むテキストを用い、Embeddingベクトルや重みを段階的に最適化していきます。
その後、モデル学習にはトークナイズ済みの大規模テキストデータ（例：数十GB〜TB） が使われ、Embedding層を含めた全パラメータが調整されます。

embedding行列が更新されていくことで、類似の文脈で使われるトークン同士が似たベクトルに近づいていく。
こうして学習が進むにつれて意味的距離を反映するようにベクトルが配置がされた空間が形成される。これが「意味空間（semantic space）」と呼ばれるもので、モデルが言語の意味的な構造を内部に持つ根本的な仕組みです。

「赤飯」と「赤面」は同じ「赤」というトークンを含む可能性があるが、分割が「赤」「飯」「面」となった場合、確かに「赤」のIDは共通する。
ただし、その「赤」というトークンのEmbeddingベクトルが「赤飯」と「赤面」のそれぞれの文脈でどれだけ使われているか、また学習データでの共起・文脈情報によって、「赤飯」と「赤面」の単語全体のEmbeddingの意味的な距離は変わる（近づくか遠ざかるか）ということ。
つまり、「赤」という共通トークンの存在だけでは意味空間上の距離は決まらず、文脈的・統計的な情報が大きく影響する。